{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c804efcd-ab48-413b-a1ae-13d88a814eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Camada Silver (Cleaned / Refined Layer)**\n",
    "\n",
    "Dados j√° limpos, validados e com estrutura√ß√£o melhorada.\n",
    "\n",
    "Normalmente inclui tratamento de erros, remo√ß√£o de duplicatas e padroniza√ß√£o.\n",
    "\n",
    "Serve para an√°lises intermedi√°rias e alimenta√ß√£o de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a48bd786-46bc-4e70-b4f0-cd45d4ad30ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3eb8d25-26c0-4035-b1cf-fa85f6b9bf57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE CATALOG pipe1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151400cb-c790-4713-acff-32e1c6db409e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Tratamento de dados da LCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b09530-cf98-4663-a011-d4a24a7c293f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "‚úÖ O que √© particionamento?\n",
    "Particionamento √© a t√©cnica de organizar fisicamente os dados em pastas separadas com base nos valores de uma ou mais colunas (ex: ano, m√™s, regi√£o etc).\n",
    "üìÇ Exemplo:\n",
    "Se voc√™ particionar uma tabela pela coluna ano, o Delta Lake vai salvar os dados assim:\n",
    "ano=2023/part-0001.parquet  \n",
    "ano=2024/part-0002.parquet  \n",
    "Se particionar por ano e mes:\n",
    "/ano=2024/mes=01/part-001.snappy.parquet  \n",
    "/ano=2024/mes=02/part-002.snappy.parquet  \n",
    "üöÄ Por que o particionamento √© importante no Delta Lake?\n",
    "O Delta Lake usa o particionamento para acelerar as consultas, pois ele:\n",
    "\n",
    "1. üîç Reduz o volume de dados lido\n",
    "Se voc√™ filtrar WHERE ano = 2024, o Spark l√™ s√≥ a pasta ano=2024, ignorando as demais.\n",
    "\n",
    "Isso acelera muito a leitura e economiza recursos de CPU/mem√≥ria.\n",
    "\n",
    "2. üßπ Organiza melhor os dados\n",
    "Ideal para dados temporais (ano, m√™s, dia), regionais (estado, cidade) ou categorias.\n",
    "\n",
    "Ajuda na manuten√ß√£o e gerenciamento do data lake.\n",
    "\n",
    "3. ‚öôÔ∏è Facilita atualiza√ß√£o incremental\n",
    "Se cada parti√ß√£o representa um m√™s, por exemplo, √© poss√≠vel atualizar apenas aquele m√™s com overwrite sem afetar os demais.\n",
    "\n",
    "4. üìä Melhora performance de ferramentas BI\n",
    "Power BI, Metabase, Superset... todas se beneficiam de datasets bem particionados (menos tempo de carregamento e resposta).\n",
    "‚ö†Ô∏è Cuidado: particionar demais tamb√©m atrapalha\n",
    "Particionar por colunas de alta cardinalidade (como CPF, ID de cliente, ou timestamps completos) cria milhares de pastas, e isso piora a performance.\n",
    "\n",
    "Sempre particione por colunas com baixo n√∫mero de valores √∫nicos e que voc√™ costuma usar em filtros.\n",
    "\n",
    "‚úÖ Resumo visual:\n",
    "Sem particionamento\tCom particionamento por ano, mes\n",
    "Spark l√™ todos os arquivos\tSpark l√™ s√≥ os arquivos relevantes\n",
    "Consulta lenta\tConsulta r√°pida\n",
    "Uso excessivo de recursos\tEconomia de CPU e mem√≥ria\n",
    "Mais dif√≠cil de manter\tOrganiza√ß√£o mais clara e modular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c139f25-1752-45b6-aa35-a6fc17d0fa9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üß† O que √© o Z-Ordering?\n",
    "√â um tipo especial de ordena√ß√£o multidimensional dos dados dentro de cada parti√ß√£o Delta. Ele reorganiza os arquivos internamente, colocando juntos os registros que compartilham valores semelhantes nas colunas mais consultadas.\n",
    "\n",
    "√â inspirado no Z-order curve, uma t√©cnica de indexa√ß√£o espacial que melhora a localidade dos dados em disco.\n",
    "\n",
    " Exemplo pr√°tico (sem Z-ordering):\n",
    "Voc√™ tem uma tabela com milh√µes de linhas e faz consultas com filtros assim:\n",
    "\n",
    "SELECT * FROM vendas WHERE cidade = 'S√£o Paulo'\n",
    "\n",
    "Se os dados est√£o desordenados, o Spark precisa ler muitos arquivos, mesmo que apenas alguns contenham 'S√£o Paulo'.\n",
    "\n",
    "‚úÖ Exemplo com Z-Ordering:\n",
    "Se voc√™ aplica:\n",
    "OPTIMIZE minha_tabela ZORDER BY (cidade)\n",
    "\n",
    "O Delta Lake reorganiza os dados internamente, agrupando os registros de 'S√£o Paulo' nos mesmos arquivos ou blocos.\n",
    "\n",
    "A pr√≥xima vez que voc√™ rodar o SELECT, o Spark ler√° menos arquivos, reduzindo o tempo e custo da consulta.\n",
    "\n",
    "üß™ Quando usar Z-Ordering?\n",
    "A tabela j√° tem particionamento por colunas como ano, mes, e voc√™ quer melhorar o desempenho de filtros por colunas n√£o particionadas (ex: cidade, produto_id, cliente_id).\n",
    "\n",
    "Voc√™ faz muitas leituras com filtros em uma ou mais colunas espec√≠ficas.\n",
    "\n",
    "A tabela tem grande volume de dados e voc√™ quer otimizar leitura sem alterar a parti√ß√£o f√≠sica.\n",
    "\n",
    "üìå Sintaxe no Databricks:\n",
    "\n",
    "OPTIMIZE minha_tabela\n",
    "ZORDER BY (coluna1, coluna2)\n",
    "\n",
    "Ou em PySpark:\n",
    "spark.sql(\"\"\"\n",
    "  OPTIMIZE pipe1.silver.slv_despesas_lcb_contemporaneo\n",
    "  ZORDER BY (condominio, tipo_despesa)\n",
    "\"\"\")\n",
    "\n",
    "‚ö†Ô∏è Observa√ß√µes:\n",
    "Item\tZ-Ordering\n",
    "N√≠vel de atua√ß√£o\tDentro de arquivos/parquet\n",
    "Altera a parti√ß√£o f√≠sica\t‚ùå N√£o\n",
    "Aumenta o custo de escrita\t‚úÖ Sim (reordena arquivos)\n",
    "Melhora leitura\t‚úÖ Muito, quando bem usado\n",
    "\n",
    "‚úÖ Conclus√£o:\n",
    "Z-Ordering = reorganiza√ß√£o interna dos dados em Delta Lake para tornar consultas filtradas mais r√°pidas.\n",
    "\n",
    "Use quando:\n",
    "\n",
    "Voc√™ tem filtros repetitivos por coluna(s).\n",
    "\n",
    "Sua tabela j√° est√° estabilizada e voc√™ quer otimizar leitura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6abd9731-702d-418e-80d3-3e66a146100c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format, round, year, month, to_timestamp, to_date, substring\n",
    "\n",
    "# 1. Ler tabela Bronze\n",
    "df_bronze = spark.table(\"pipe1.bronze.brz_despesas_lcb_contemporaneo\")\n",
    "\n",
    "# 2. Remover a coluna _line\n",
    "df = df_bronze.drop(\"_line\")\n",
    "\n",
    "# 3. Transformar _fivetran_synced para string com timestamp formatado\n",
    "df = df.withColumn(\"_fivetran_synced\", date_format(col(\"_fivetran_synced\"), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "# 4. Garantir que 'periodo' esteja no formato string \"yyyy-MM\"\n",
    "# (caso venha como date ou timestamp, usamos date_format)\n",
    "df = df.withColumn(\"periodo\", date_format(col(\"periodo\"), \"yyyy-MM\"))\n",
    "\n",
    "# 5. Arredondar colunas num√©ricas\n",
    "df = df.withColumn(\"kwh_dia\", round(col(\"kwh_dia\"), 2))\n",
    "df = df.withColumn(\"total_agua\", round(col(\"total_agua\"), 2))\n",
    "\n",
    "# 6. Substituir valores nulos por 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "# 7. Criar colunas de parti√ß√£o ano e m√™s extra√≠das da string 'periodo'\n",
    "df = df.withColumn(\"ano\", substring(col(\"periodo\"), 1, 4).cast(\"int\")) \\\n",
    "       .withColumn(\"mes\", substring(col(\"periodo\"), 6, 2).cast(\"int\"))\n",
    "\n",
    "# 8. Salvar como Delta particionado por ano e m√™s\n",
    "df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"ano\", \"mes\") \\\n",
    "    .saveAsTable(\"pipe1.silver.slv_despesas_lcb_contemporaneo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4bee1c-8986-449b-866f-b234189d639f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dropar a tabela\n",
    "#spark.sql(\"DROP TABLE IF EXISTS pipe1.silver.slv_despesas_lcb_contemporaneo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f8393c-b079-41de-809b-8d44800f66fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = spark.table(\"pipe1.silver.slv_despesas_lcb_contemporaneo\")\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c42b63-d60a-4210-a229-803349292777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Tratamento de dados da Timbauvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9948cd4-1f23-4a5c-8896-cd67a594e487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from workspace.google_drive.dados_despesas_home_despesa_timbauvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65ec8f91-05a5-444d-a51e-b4f9e80f8fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_timestamp, date_format, round, substring\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# 1. Carregar dados da camada Bronze\n",
    "df = spark.table(\"pipe1.bronze.brz_despesas_timbauvas\")\n",
    "\n",
    "# 2. Remover a coluna _line\n",
    "df = df.drop(\"_line\")\n",
    "\n",
    "# 3. Transformar _fivetran_synced para yyyy-MM-dd e timestamp formatado\n",
    "df = df.withColumn(\"_fivetran_synced\", date_format(col(\"_fivetran_synced\"), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "# 4. Alterar a coluna 'periodo' para string no formato ano-m√™s (\"yyyy-MM\")\n",
    "df = df.withColumn(\"periodo\", date_format(to_timestamp(col(\"periodo\")), \"yyyy-MM\"))\n",
    "\n",
    "# 5. Arredondar kwh_dia para 2 casas decimais\n",
    "df = df.withColumn(\"kwh_dia\", round(col(\"kwh_dia\"), 2))\n",
    "\n",
    "# 6. Converter despesa_de_luz para n√∫mero com 2 casas decimais\n",
    "df = df.withColumn(\"despesa_de_luz\", round(col(\"despesa_de_luz\").cast(DoubleType()), 2))\n",
    "\n",
    "# 7. Substituir todos os valores nulos por 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "# 8. Arredondar consumo_kg para 2 casas decimais\n",
    "df = df.withColumn(\"consumo_de_agua\", round(col(\"consumo_de_agua\"), 2))\n",
    "\n",
    "# 9. Arredondar total_agua para 2 casas decimais\n",
    "df = df.withColumn(\"total_agua\", round(col(\"total_agua\"), 2))\n",
    "\n",
    "# 10. Criar colunas 'ano' e 'mes' extra√≠das da string 'periodo'\n",
    "df = df.withColumn(\"ano\", substring(col(\"periodo\"), 1, 4).cast(\"int\")) \\\n",
    "       .withColumn(\"mes\", substring(col(\"periodo\"), 6, 2).cast(\"int\"))\n",
    "\n",
    "# 11. Gravar na camada Silver particionado por ano e m√™s\n",
    "df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"ano\", \"mes\") \\\n",
    "    .saveAsTable(\"pipe1.silver.slv_despesas_timbauvas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3076be8-42d3-4d39-b18b-35fd0af231ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = spark.table(\"pipe1.silver.slv_despesas_timbauvas\")\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13deb7c9-ef0d-4a49-8d05-bdf04c45330c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dropar a tabela\n",
    "#spark.sql(\"DROP TABLE IF EXISTS pipe1.silver.slv_despesas_timbauvas\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5930585342559401,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Camada Silver 2. Tratar dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
